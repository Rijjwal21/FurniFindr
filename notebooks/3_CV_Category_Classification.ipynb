{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c5ef7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0588d4bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_PATH = \"../backend/app/data_ingestion/sample_data.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# Clean categories and images\n",
    "def safe_literal_eval(val):\n",
    "    if not isinstance(val, str) or not val.startswith('['):\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "df['categories_clean'] = df['categories'].apply(safe_literal_eval)\n",
    "df['images_clean'] = df['images'].apply(safe_literal_eval)\n",
    "\n",
    "# Get the first image URL\n",
    "df['image_url'] = df['images_clean'].apply(lambda x: x[0] if len(x) > 0 else None)\n",
    "\n",
    "# --- Map complex categories to simple ones --- \n",
    "def map_category(cats):\n",
    "    for cat in cats:\n",
    "        cat = str(cat).lower()\n",
    "        if 'chair' in cat:\n",
    "            return 'Chair'\n",
    "        if 'table' in cat or 'desk' in cat:\n",
    "            return 'Table/Desk'\n",
    "        if 'bookcase' in cat or 'tv stand' in cat:\n",
    "            return 'Storage'\n",
    "        if 'bed' in cat:\n",
    "            return 'Bed'\n",
    "    return 'Other'\n",
    "\n",
    "df['simple_category'] = df['categories_clean'].apply(map_category)\n",
    "\n",
    "# Filter for items that have an image and a non-'Other' category\n",
    "df_cv = df[~df['image_url'].isnull() & (df['simple_category'] != 'Other')].copy()\n",
    "\n",
    "# Create integer labels for our categories\n",
    "labels = df_cv['simple_category'].unique()\n",
    "label_map = {label: i for i, label in enumerate(labels)}\n",
    "label_map_inv = {i: label for label, i in label_map.items()}\n",
    "df_cv['label'] = df_cv['simple_category'].map(label_map)\n",
    "\n",
    "N_CLASSES = len(labels)\n",
    "print(f\"Found {len(df_cv)} usable images across {N_CLASSES} classes.\")\n",
    "print(f\"Classes: {label_map}\")\n",
    "df_cv[['title', 'image_url', 'simple_category', 'label']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6801bb5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define transformations for ResNet18\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class FurnitureDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_url = row['image_url']\n",
    "        label = torch.tensor(row['label'], dtype=torch.long)\n",
    "\n",
    "        try:\n",
    "            # Download and open image\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            response = requests.get(image_url, timeout=5, headers=headers)\n",
    "            response.raise_for_status() # Raise error for bad responses\n",
    "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, label\n",
    "        \n",
    "        except Exception as e:\n",
    "            # print(f\"Warning: Could not load image {image_url}. {e}\")\n",
    "            # Return a blank tensor and a special label (-1) to skip it\n",
    "            return torch.zeros((3, 224, 224)), torch.tensor(-1, dtype=torch.long)\n",
    "\n",
    "# Split the data (using a small test set for demo)\n",
    "train_df, test_df = train_test_split(df_cv, test_size=0.4, random_state=42, stratify=df_cv['label'])\n",
    "\n",
    "train_dataset = FurnitureDataset(train_df, transform=data_transforms)\n",
    "test_dataset = FurnitureDataset(test_df, transform=data_transforms)\n",
    "\n",
    "# Custom collate_fn to filter out failed image loads\n",
    "def collate_fn(batch):\n",
    "    batch = [(img, lbl) for img, lbl in batch if lbl != -1]\n",
    "    if not batch:\n",
    "        return torch.empty(0), torch.empty(0)\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 2 # Very small for demo\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Created DataLoaders. Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c624b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load a pre-trained ResNet18 model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Get the number of input features for the final layer\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# Replace the final fully connected layer\n",
    "model.fc = nn.Linear(num_ftrs, N_CLASSES)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded. Final layer replaced to output {N_CLASSES} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc9652",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# We only optimize the parameters of the new final layer\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "NUM_EPOCHS = 3 # Keep low for demo\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        if images.nelement() == 0: continue # Skip empty batches (from failed downloads)\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # We check len(train_dataset.df) because len(train_dataset) is affected by failed loads\n",
    "    epoch_loss = running_loss / len(train_dataset.df)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcaf6b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.eval() # Set model to evaluation mode\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "with torch.no_grad(): # No gradients needed\n",
    "    for images, labels in test_loader:\n",
    "        if images.nelement() == 0: continue\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1) # Get the index of the max logit\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f725b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Classification Report (Precision, Recall, F1-Score)\n",
    "if len(all_labels) > 0:\n",
    "    # Ensure all labels are present in the map for the report\n",
    "    unique_labels_in_data = [l for l in np.unique(all_labels + all_preds) if l in label_map_inv]\n",
    "    target_names = [label_map_inv[i] for i in unique_labels_in_data]\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=target_names, zero_division=0))\n",
    "else:\n",
    "    print(\"No valid test data to evaluate.\")\n",
    "\n",
    "print(\"\\nEvaluation: This report shows the model's performance on the test set.\")\n",
    "print(\"(Note: Metrics will be poor due to the tiny dataset, but this demonstrates the process)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbcee72",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Confusion Matrix\n",
    "if len(all_labels) > 0:\n",
    "    unique_labels_in_data = [l for l in np.unique(all_labels + all_preds) if l in label_map_inv]\n",
    "    target_names = [label_map_inv[i] for i in unique_labels_in_data]\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=unique_labels_in_data)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=target_names, \n",
    "                yticklabels=target_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot show confusion matrix: no test data.\")\n",
    "    \n",
    "print(\"\\nEvaluation: This matrix shows which classes the model is confusing (e.t., predicting 'Table' when it was a 'Chair').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e5ef6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the artifacts directory exists\n",
    "os.makedirs(\"./artifacts\", exist_ok=True)\n",
    "MODEL_SAVE_PATH = \"./artifacts/cv_category_model.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Model state saved to {MODEL_SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
